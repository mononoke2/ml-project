{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12320692,"sourceType":"datasetVersion","datasetId":7766120},{"sourceId":12332942,"sourceType":"datasetVersion","datasetId":7774411},{"sourceId":12390032,"sourceType":"datasetVersion","datasetId":7812846},{"sourceId":12389853,"sourceType":"datasetVersion","datasetId":7812713}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### N.B. Run with a GPU device active ","metadata":{}},{"cell_type":"markdown","source":"# Modelli YOLO","metadata":{}},{"cell_type":"markdown","source":"Import di librerie necessarie","metadata":{}},{"cell_type":"code","source":"!pip install ultralytics","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# YOLOv8m ","metadata":{}},{"cell_type":"code","source":"from ultralytics import YOLO\n\nmodel = YOLO('yolov8m.pt')\n\nmodel.train(\n    data = \"/kaggle/input/new-dataset-for-ml-project-last-version/new-dataset-for-ml-project-last-version/data.yaml\",\n    imgsz = (640,640),\n    batch = 32,\n    optimizer = \"Adam\",\n    lr0= 1e-3\n)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"results = model.val(data = \"/kaggle/input/test-set-for-ml-project/test-set-for-ml-project/data.yaml\", split = \"test\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Validation","metadata":{}},{"cell_type":"code","source":"results.results_dict","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# YOLOv10n","metadata":{}},{"cell_type":"code","source":"from ultralytics import YOLO\n\nmodel = YOLO('yolov10n.pt')\n\nmodel.train(\n    data = \"/kaggle/input/new-dataset-for-ml-project-last-version/new-dataset-for-ml-project-last-version/data.yaml\",\n    imgsz = (640,640),\n    batch = 32,\n    optimizer = \"Adam\",\n    lr0= 1e-3\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Validation","metadata":{}},{"cell_type":"code","source":"results = model.val(data = \"/kaggle/input/test-set-for-ml-project/test-set-for-ml-project/data.yaml\", split = \"test\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"results.results_dict","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# YOLOv11s","metadata":{}},{"cell_type":"code","source":"from ultralytics import YOLO\n\nmodel = YOLO('/kaggle/input/yolo11/pytorch/default/1/yolo11s.pt') # bisogna importare i modelli Ultralytics dal notebook ufficiale di riferimento per YOLOv11 qui presente sulla piattaforma\n\nmodel.train(\n    data = \"/kaggle/input/new-dataset-for-ml-project-last-version/new-dataset-for-ml-project-last-version/data.yaml\",\n    imgsz = (640,640),\n    batch = 32,\n    optimizer = \"Adam\",\n    lr0= 1e-3\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Validation","metadata":{}},{"cell_type":"code","source":"results = model.val(data = \"/kaggle/input/test-set-for-ml-project/test-set-for-ml-project/data.yaml\", split = \"test\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"results.results_dict","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Faster R-CNN con backbone ResNet50","metadata":{}},{"cell_type":"markdown","source":"Import di librerie necessarie","metadata":{}},{"cell_type":"code","source":"import os\nimport json\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom PIL import Image\nimport cv2\n\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision\nfrom torchvision import transforms\nfrom torchvision.models.detection import fasterrcnn_resnet50_fpn\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n\nfrom pycocotools.coco import COCO\n\n# Progress bar\nfrom tqdm.auto import tqdm","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nINPUT_PATH = \"/kaggle/input/new-dataset-for-ml-project-coco\"\nWORKING_PATH = \"/kaggle/working\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class COCOCustomDataset(Dataset):\n\n    def __init__(self, root_dir, annotation_file, transforms=None):\n\n        self.root_dir = root_dir\n        self.transforms = transforms\n        \n        # Load COCO annotations\n        self.coco = COCO(annotation_file)\n        \n        # Get all image ids\n        self.image_ids = list(self.coco.imgs.keys())\n        \n        # Get category information\n        self.category_ids = list(self.coco.cats.keys())\n        self.category_names = [self.coco.cats[cat_id]['name'] for cat_id in self.category_ids]\n    \n    def __len__(self):\n        return len(self.image_ids)\n    \n    def __getitem__(self, idx):\n        # Get image info\n        img_id = self.image_ids[idx]\n        img_info = self.coco.imgs[img_id]\n        img_path = os.path.join(self.root_dir, img_info['file_name'])\n        \n        # Load image\n        image = Image.open(img_path).convert(\"RGB\")\n        image = np.array(image)\n        \n        # Get annotations for this image\n        ann_ids = self.coco.getAnnIds(imgIds=img_id)\n        anns = self.coco.loadAnns(ann_ids)\n        \n        boxes = []\n        labels = []\n        areas = []\n        iscrowd = []\n        \n        for ann in anns:\n            # COCO bbox format: [x, y, width, height]\n            # Convert to [x1, y1, x2, y2]\n            x, y, w, h = ann['bbox']\n            boxes.append([x, y, x + w, y + h])\n            \n            # Map category_id to label (0-indexed for model)\n            # For pedestrian detection: map all to class 1 (person)\n            labels.append(1)  # 1 for person, 0 is background\n            \n            areas.append(ann['area'])\n            iscrowd.append(ann['iscrowd'])\n        \n        # Convert to tensors\n        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n        labels = torch.as_tensor(labels, dtype=torch.int64)\n        areas = torch.as_tensor(areas, dtype=torch.float32)\n        iscrowd = torch.as_tensor(iscrowd, dtype=torch.int64)\n        \n        # Create target dictionary\n        target = {\n            \"boxes\": boxes,\n            \"labels\": labels,\n            \"area\": areas,\n            \"iscrowd\": iscrowd,\n            \"image_id\": torch.tensor([img_id])\n        }\n        \n        # Apply transforms\n        if self.transforms:\n            image, target = self.transforms(image, target)\n        else:\n            image = torch.from_numpy(image).permute(2, 0, 1).float() / 255.0\n        \n        return image, target\n\n\ndef collate_fn(batch):\n    \"\"\"Custom collate function for DataLoader\"\"\"\n    images, targets = tuple(zip(*batch))\n    return list(images), list(targets)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Data transforms and augmentation (references/detection/transforms.py)\nclass Compose:\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, image, target):\n        for t in self.transforms:\n            image, target = t(image, target)\n        return image, target\n\n\nclass ToTensor:\n    def __call__(self, image, target):\n        image = torch.from_numpy(image).permute(2, 0, 1).float() / 255.0\n        return image, target\n\n\nclass Normalize:\n    def __init__(self, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n        self.mean = mean\n        self.std = std\n\n    def __call__(self, image, target):\n        for i in range(3):\n            image[i] = (image[i] - self.mean[i]) / self.std[i]\n        return image, target\n\n\nclass RandomHorizontalFlip:\n    def __init__(self, prob=0.5):\n        self.prob = prob\n\n    def __call__(self, image, target):\n        if torch.rand(1) < self.prob:\n            height, width = image.shape[-2:]\n            image = image.flip(-1)\n            bbox = target[\"boxes\"]\n            bbox[:, [0, 2]] = width - bbox[:, [2, 0]]\n            target[\"boxes\"] = bbox\n        return image, target\n\n\ndef get_transforms(train = True):\n    transforms_list = []\n    transforms_list.append(ToTensor())\n    \n    if train:\n        transforms_list.append(RandomHorizontalFlip(0.5))\n    \n    transforms_list.append(Normalize())\n    return Compose(transforms_list)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"DATASET_NAME = \"new-dataset-for-ml-project-coco\" \n\ntrain_dir = f\"{INPUT_PATH}/{DATASET_NAME}/train\"\nvalid_dir = f\"{INPUT_PATH}/{DATASET_NAME}/valid\"\ntest_dir = f\"/kaggle/input/test-set-for-ml-project-coco/test\"\n\n\ntrain_dataset = COCOCustomDataset(\n    root_dir=train_dir,\n    annotation_file=os.path.join(train_dir, \"_annotations.coco.json\"),\n    transforms=get_transforms(train=True)\n)\n    \nval_dataset = COCOCustomDataset(\n    root_dir=valid_dir,\n    annotation_file=os.path.join(valid_dir, \"_annotations.coco.json\"),\n    transforms=get_transforms(train=False)\n)\n    \ntest_dataset = COCOCustomDataset(\n    root_dir=test_dir,\n    annotation_file=os.path.join(test_dir, \"_annotations.coco.json\"),\n    transforms=get_transforms(train=False)\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"BATCH_SIZE = 4 \nNUM_WORKERS = 2  \n\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    num_workers=NUM_WORKERS,\n    collate_fn=collate_fn\n)\n\nval_loader = DataLoader(\n    val_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=False,\n    num_workers=NUM_WORKERS,\n    collate_fn=collate_fn\n)\n\ntest_loader = DataLoader(\n    test_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=False,\n    num_workers=NUM_WORKERS,\n    collate_fn=collate_fn\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize the model\nprint(\"Initializing Faster R-CNN with ResNet50 backbone...\")\n\n\nnum_classes = 2 # person + background\n        \nmodel = fasterrcnn_resnet50_fpn(\n    pretrained = True,\n    progress = True,\n    pretrained_backbone = True\n)\n        \nin_features = model.roi_heads.box_predictor.cls_score.in_features\nmodel.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n\nmodel.to(device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Configure training parameters\nLEARNING_RATE = 0.001\nWEIGHT_DECAY = 0.0005\nMOMENTUM = 0.9\nNUM_EPOCHS = 30 \nSTEP_SIZE = 3\nGAMMA = 0.1\n\n# Setup optimizer and scheduler\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = optim.SGD(\n    params, \n    lr = LEARNING_RATE,\n    momentum = MOMENTUM, \n    weight_decay = WEIGHT_DECAY\n)\n\nlr_scheduler = optim.lr_scheduler.StepLR(\n    optimizer,\n    step_size = STEP_SIZE,\n    gamma = GAMMA\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Training functions (reference/detection/engine.py)\ndef train_one_epoch(model, optimizer, data_loader, device, epoch):\n    \"\"\"Train for one epoch\"\"\"\n    model.train()\n    total_loss = 0\n    num_batches = len(data_loader)\n    \n    progress_bar = tqdm(data_loader, desc=f\"Training Epoch {epoch+1}\")\n    \n    for batch_idx, (images, targets) in enumerate(progress_bar):\n        # Move data to device\n        images = [img.to(device) for img in images]\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n        \n        # Forward pass\n        loss_dict = model(images, targets)\n        losses = sum(loss for loss in loss_dict.values())\n        \n        # Backward pass\n        optimizer.zero_grad()\n        losses.backward()\n        optimizer.step()\n        \n        total_loss += losses.item()\n        \n        # Update progress bar\n        progress_bar.set_postfix({\n            'Loss': f'{losses.item():.4f}',\n            'Avg Loss': f'{total_loss/(batch_idx+1):.4f}'\n        })\n    \n    return total_loss / num_batches\n\n\ndef validate_one_epoch(model, data_loader, device, epoch):\n    \"\"\"Validate for one epoch\"\"\"\n    model.train()   # va lasciato in train poiché faster rcnn ritorna il dizionario con le loss solo in modalità train, in modalità eval restituisce direttamente la lista di predizioni\n    total_loss = 0\n    num_batches = len(data_loader)\n    \n    with torch.no_grad():\n        progress_bar = tqdm(data_loader, desc=f\"Validation Epoch {epoch+1}\")\n        \n        for batch_idx, (images, targets) in enumerate(progress_bar):\n            # Move data to device\n            images = [img.to(device) for img in images]\n            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n            \n            # Forward pass\n            loss_dict = model(images, targets)\n            losses = sum(loss for loss in loss_dict.values())\n            \n            total_loss += losses.item()\n            \n            # Update progress bar\n            progress_bar.set_postfix({\n                'Loss': f'{losses.item():.4f}',\n                'Avg Loss': f'{total_loss/(batch_idx+1):.4f}'\n            })\n    \n    return total_loss / num_batches","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Main training loop\nprint(\"Starting training...\")\nprint(\"=\" * 60)\n\n# Training history\ntrain_losses = []\nval_losses = []\nbest_val_loss = float('inf')\n\n# Create output directory\nos.makedirs(f\"{WORKING_PATH}/checkpoints\", exist_ok=True)\n\nfor epoch in range(NUM_EPOCHS):\n    print(f\"\\n Epoch {epoch+1}/{NUM_EPOCHS}\")\n    print(\"-\" * 40)\n    \n    # Training phase\n    train_loss = train_one_epoch(model, optimizer, train_loader, device, epoch)\n    train_losses.append(train_loss)\n    \n    # Validation phase\n    val_loss = validate_one_epoch(model, val_loader, device, epoch)\n    val_losses.append(val_loss)\n    \n    # Update learning rate\n    lr_scheduler.step()\n    current_lr = optimizer.param_groups[0]['lr']\n    \n    # Print epoch results\n    print(f\"\\n Epoch {epoch+1} Results:\")\n    print(f\" Train Loss: {train_loss:.4f}\")\n    print(f\" Val Loss: {val_loss:.4f}\")\n    print(f\" Learning Rate: {current_lr:.6f}\")\n    \n    # Save best model\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        torch.save(model.state_dict(), f\"{WORKING_PATH}/checkpoints/best_model.pth\")\n    \n# Save final model\ntorch.save(model.state_dict(), f\"{WORKING_PATH}/checkpoints/final_model.pth\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluate_model(model, data_loader, device, confidence_threshold = 0.5):\n    model.eval()\n    \n    all_predictions = []\n    all_targets = []\n    \n    with torch.no_grad():\n        for images, targets in tqdm(data_loader, desc=\"Evaluating\"):\n            images = [img.to(device) for img in images]\n            \n            predictions = model(images)\n            \n            # Process predictions and targets\n            for pred, target in zip(predictions, targets):\n                scores = pred['scores']\n                keep = scores >= confidence_threshold\n                \n                pred_filtered = {\n                    'boxes': pred['boxes'][keep].cpu().numpy(),\n                    'scores': pred['scores'][keep].cpu().numpy(),\n                    'labels': pred['labels'][keep].cpu().numpy()\n                }\n                \n                target_processed = {\n                    'boxes': target['boxes'].cpu().numpy(),\n                    'labels': target['labels'].cpu().numpy()\n                }\n                \n                all_predictions.append(pred_filtered)\n                all_targets.append(target_processed)\n    \n    return all_predictions, all_targets\n\n\ndef calculate_iou(box1, box2):\n    # Calculate intersection coordinates\n    x1 = max(box1[0], box2[0])\n    y1 = max(box1[1], box2[1])\n    x2 = min(box1[2], box2[2])\n    y2 = min(box1[3], box2[3])\n    \n    # Calculate intersection area\n    if x2 <= x1 or y2 <= y1:\n        return 0.0\n    \n    intersection = (x2 - x1) * (y2 - y1)\n    \n    # Calculate union area\n    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n    union = area1 + area2 - intersection\n    \n    return intersection / union if union > 0 else 0.0\n\n\ndef calculate_metrics(predictions, targets, iou_threshold = 0.5):\n    all_pred_boxes = []\n    all_pred_scores = []\n    all_gt_boxes = []\n    \n    # Collect all predictions and ground truths\n    for pred, target in zip(predictions, targets):\n        all_pred_boxes.extend(pred['boxes'])\n        all_pred_scores.extend(pred['scores'])\n        all_gt_boxes.extend(target['boxes'])\n    \n    # Sort predictions by confidence score\n    sorted_indices = np.argsort(all_pred_scores)[::-1]\n    sorted_pred_boxes = [all_pred_boxes[i] for i in sorted_indices]\n    \n    # Calculate matches\n    tp = 0  # True positives\n    fp = 0  # False positives\n    matched_gt = set()  # Keep track of matched ground truth boxes\n    \n    for pred_box in sorted_pred_boxes:\n        best_iou = 0.0\n        best_gt_idx = -1\n        \n        for gt_idx, gt_box in enumerate(all_gt_boxes):\n            if gt_idx in matched_gt:\n                continue\n            \n            iou = calculate_iou(pred_box, gt_box)\n            if iou > best_iou:\n                best_iou = iou\n                best_gt_idx = gt_idx\n        \n        if best_iou >= iou_threshold:\n            tp += 1\n            matched_gt.add(best_gt_idx)\n        else:\n            fp += 1\n    \n    fn = len(all_gt_boxes) - tp  # False negatives\n\n    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n    \n    return {\n        'precision': precision,\n        'recall': recall,\n        'f1_score': f1_score,\n        'true_positives': tp,\n        'false_positives': fp,\n        'false_negatives': fn,\n        'total_predictions': len(all_pred_boxes),\n        'total_ground_truth': len(all_gt_boxes)\n    }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Validation","metadata":{}},{"cell_type":"code","source":"# Evaluate the trained model\nprint(\"Evaluating model on test set...\")\nprint(\"=\" * 50)\n\n# Load best model\nmodel.load_state_dict(torch.load(f\"{WORKING_PATH}/checkpoints/best_model.pth\", map_location = device))\n\n# Evaluate on test set\ntest_predictions, test_targets = evaluate_model(model, test_loader, device, confidence_threshold=0.5)\n\n# Calculate metrics\ntest_metrics = calculate_metrics(test_predictions, test_targets, iou_threshold=0.5)\n\nprint(\"\\nTest Set Evaluation Results:\")\nprint(\"-\" * 30)\nprint(f\" Precision: {test_metrics['precision']:.4f}\")\nprint(f\" Recall: {test_metrics['recall']:.4f}\")\nprint(f\" F1-Score: {test_metrics['f1_score']:.4f}\")\nprint(f\" True Positives: {test_metrics['true_positives']}\")\nprint(f\" False Positives: {test_metrics['false_positives']}\")\nprint(f\" False Negatives: {test_metrics['false_negatives']}\")\nprint(f\" Total Predictions: {test_metrics['total_predictions']}\")\nprint(f\" Total Ground Truth: {test_metrics['total_ground_truth']}\")\n\n# Also evaluate on validation set for comparison\nprint(\"\\nEvaluating on validation set for comparison...\")\nval_predictions, val_targets = evaluate_model(model, val_loader, device, confidence_threshold=0.5)\nval_metrics = calculate_metrics(val_predictions, val_targets, iou_threshold=0.5)\n\nprint(\"\\nValidation Set Evaluation Results:\")\nprint(\"-\" * 30)\nprint(f\" Precision: {val_metrics['precision']:.4f}\")\nprint(f\" Recall: {val_metrics['recall']:.4f}\")\nprint(f\" F1-Score: {val_metrics['f1_score']:.4f}\")\nprint(f\" True Positives: {val_metrics['true_positives']}\")\nprint(f\" False Positives: {val_metrics['false_positives']}\")\nprint(f\" False Negatives: {val_metrics['false_negatives']}\")\nprint(f\" Total Predictions: {val_metrics['total_predictions']}\")\nprint(f\" Total Ground Truth: {val_metrics['total_ground_truth']}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}