{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12387731,"sourceType":"datasetVersion","datasetId":7811259}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Demo 4 ML Project ","metadata":{}},{"cell_type":"markdown","source":"**#1** Upload a .png/.jpg file on a new dataset in order to predict a frame image. You can also use .mov/.mp4/.avi type in order to predict video. <br>\n**NB:** For reliable results your image/video must be of the size used in dataset for training which is **640x640**.","metadata":{}},{"cell_type":"markdown","source":"**#2** You need to install Ultralytics library in order to execute YOLO model(s).","metadata":{}},{"cell_type":"code","source":"!pip install ultralytics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T11:14:56.876660Z","iopub.execute_input":"2025-07-06T11:14:56.877194Z","iopub.status.idle":"2025-07-06T11:15:02.514267Z","shell.execute_reply.started":"2025-07-06T11:14:56.877157Z","shell.execute_reply":"2025-07-06T11:15:02.512961Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Image mode - YOLO","metadata":{}},{"cell_type":"code","source":"from ultralytics import YOLO\nfrom PIL import Image\n\nmodel = YOLO(\"/kaggle/input/best-weights-for-model/one-of-the-yolo-models\")\nimg = Image.open(INPUT_FILE_PATH)\n\n\nres = model.predict(img)[0]\n\nprint(\"Total detections: \" + str(len(res)))\n\nres = res.plot(line_width=1)\nres = res[:, :, ::-1]\n\nres = Image.fromarray(res)\nres.save(\"output.png\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#NB: You can also specify the threshold confidence by adding to the predict function the parameter conf = 0.5 or the value you desire. By default it should be 0.25 or 0.5 it depends on YOLO version. ","metadata":{}},{"cell_type":"markdown","source":"# Video mode - YOLO","metadata":{}},{"cell_type":"code","source":"from ultralytics import YOLO \n\nmodel = YOLO(\"/kaggle/input/best-weights-for-model/best yolov8m.pt\")\nresults = model.predict(source = INPUT_FILE_PATH, save = True, project = 'runs/detect', name = 'exp')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T11:16:01.128586Z","iopub.execute_input":"2025-07-06T11:16:01.128991Z","iopub.status.idle":"2025-07-06T11:31:24.803067Z","shell.execute_reply.started":"2025-07-06T11:16:01.128952Z","shell.execute_reply":"2025-07-06T11:31:24.801887Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Average of People in Video\ntotal_boxes = 0.0\nsum_boxes = 0.0\n\nfor result in results:\n    boxes = result.boxes  \n    sum_boxes += len(boxes)\n    total_boxes += 1\n    \navg_detection = sum_boxes/total_boxes\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T11:32:48.337087Z","iopub.execute_input":"2025-07-06T11:32:48.337816Z","iopub.status.idle":"2025-07-06T11:32:48.346695Z","shell.execute_reply.started":"2025-07-06T11:32:48.337783Z","shell.execute_reply":"2025-07-06T11:32:48.345324Z"},"jupyter":{"source_hidden":true},"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(round(avg_detection))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T11:32:54.210844Z","iopub.execute_input":"2025-07-06T11:32:54.211239Z","iopub.status.idle":"2025-07-06T11:32:54.217618Z","shell.execute_reply.started":"2025-07-06T11:32:54.211216Z","shell.execute_reply":"2025-07-06T11:32:54.216361Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**#3** Now you can make predictions using Faster RCNN with backbone Resnet50 method. ","metadata":{}},{"cell_type":"code","source":"# Initialize the model\n\nfrom torchvision.models.detection import fasterrcnn_resnet50_fpn\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n\n\nprint(\"Initializing Faster R-CNN with ResNet50 backbone...\")\n\n\nnum_classes = 2 # person + background\n        \nmodel = fasterrcnn_resnet50_fpn(\n    pretrained = True,\n    progress = True,\n    pretrained_backbone = True\n)\n        \nin_features = model.roi_heads.box_predictor.cls_score.in_features\nmodel.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n\nmodel.to(device)\nmodel.load_state_dict(torch.load(f\"/kaggle/input/best-weights-for-model/best resnet50.pth\", map_location = device))","metadata":{"trusted":true,"jupyter":{"source_hidden":true,"outputs_hidden":true},"execution":{"iopub.status.busy":"2025-07-06T11:34:38.856224Z","iopub.execute_input":"2025-07-06T11:34:38.857359Z","iopub.status.idle":"2025-07-06T11:34:43.208040Z","shell.execute_reply.started":"2025-07-06T11:34:38.857314Z","shell.execute_reply":"2025-07-06T11:34:43.206899Z"},"collapsed":true,"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Import necessary library\n\nimport os\nimport cv2\nimport numpy as np\nimport torchvision\nfrom torchvision import transforms\nfrom PIL import Image\n# Progress bar\nfrom tqdm.auto import tqdm","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Image mode - Faster RCNN w/ Resnet 50","metadata":{}},{"cell_type":"code","source":"# Single Image Prediction and Visualization Functions\n\ndef predict_image(model, image_path, device, confidence_threshold=0.5):\n\n    # Load and preprocess image\n    image = Image.open(image_path).convert('RGB')\n    image_array = np.array(image)\n    \n    # Convert to tensor and normalize\n    transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n    \n    image_tensor = transform(image).unsqueeze(0).to(device)\n    \n    # Set model to evaluation mode\n    model.eval()\n    \n    with torch.no_grad():\n        predictions = model(image_tensor)\n    \n    # Process predictions\n    pred = predictions[0]\n    scores = pred['scores'].cpu().numpy()\n    boxes = pred['boxes'].cpu().numpy()\n    labels = pred['labels'].cpu().numpy()\n    \n    # Filter by confidence threshold\n    keep = scores >= confidence_threshold\n    \n    filtered_predictions = {\n        'boxes': boxes[keep],\n        'scores': scores[keep],\n        'labels': labels[keep],\n        'original_image': image_array\n    }\n    \n    return filtered_predictions\n\n\ndef visualize_prediction(predictions, save_path=None, figsize=(12, 8)):\n    \n    image = predictions['original_image']\n    boxes = predictions['boxes']\n    scores = predictions['scores']\n    \n    fig, ax = plt.subplots(1, 1, figsize=figsize)\n    ax.imshow(image)\n    \n    ax.set_title(f'People Detection \\nDetections: {len(boxes)}', \n                fontsize=14, fontweight='bold')\n    ax.axis('off')\n    \n    # Color scheme for boxes\n    colors = ['red', 'blue']\n    \n    # Draw prediction boxes\n    for i, (box, score) in enumerate(zip(boxes, scores)):\n        x1, y1, x2, y2 = box\n        color = colors[i % len(colors)]\n        \n        # Draw bounding box\n        rect = patches.Rectangle(\n            (x1, y1), x2-x1, y2-y1,\n            linewidth=1, edgecolor=color, facecolor='none'\n        )\n        ax.add_patch(rect)\n        \n        # Add confidence label\n        label = f'{score:.3f}'\n        ax.text(x1, y1-10, label, \n               bbox=dict(facecolor=color, alpha=0.7),\n               color='white', fontsize=6, weight='bold')\n    \n    # Add detection summary\n    if len(boxes) > 0:\n        summary_text = f\"Detections: {len(boxes)}\\n\"\n        summary_text += f\"Confidence range: {scores.min():.3f} - {scores.max():.3f}\\n\"\n        summary_text += f\"Avg confidence: {scores.mean():.3f}\"\n        \n        ax.text(0.02, 0.98, summary_text,\n               transform=ax.transAxes, fontsize=10,\n               bbox=dict(boxstyle=\"round,pad=0.5\", facecolor='white', alpha=0.8),\n               verticalalignment='top')\n    else:\n        ax.text(0.02, 0.98, \"No detections found\",\n               transform=ax.transAxes, fontsize=12, color='red',\n               bbox=dict(boxstyle=\"round,pad=0.5\", facecolor='white', alpha=0.8),\n               verticalalignment='top', weight='bold')\n    \n    plt.tight_layout()\n    \n    if save_path:\n        plt.savefig(save_path, dpi = 300, bbox_inches = 'tight')\n        print(f\"Visualization saved to: {save_path}\")\n    \n    plt.show()\n\n\ndef predict_and_visualize(model, image_path, device, confidence_threshold = 0.5, save_path = None):\n    \n    # Make predictions\n    predictions = predict_image(model, image_path, device, confidence_threshold)\n      \n    # Visualize results\n    visualize_prediction(predictions, save_path)\n        \n    return predictions","metadata":{"trusted":true,"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-07-06T11:39:30.226669Z","iopub.execute_input":"2025-07-06T11:39:30.227088Z","iopub.status.idle":"2025-07-06T11:39:30.245134Z","shell.execute_reply.started":"2025-07-06T11:39:30.227060Z","shell.execute_reply":"2025-07-06T11:39:30.243800Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predictions = predict_and_visualize(\n    model=model,\n    image_path=INPUT_FILE_PATH,\n    device=device,\n    confidence_threshold = 0.5\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Video mode - Faster RCNN w/ Resnet 50","metadata":{}},{"cell_type":"code","source":"# Video Prediction Functions\n\ndef predict_video(model, video_path, device, confidence_threshold = 0.5, \n                 output_path = None, skip_frames = 0, max_frames = None):\n    \n    # Open video\n    cap = cv2.VideoCapture(video_path)\n\n    fps = int(cap.get(cv2.CAP_PROP_FPS))\n    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n\n    # Create temporary directory for frames\n    temp_dir = f\"{WORKING_PATH}/temp_video_frames\"\n    os.makedirs(temp_dir, exist_ok=True)\n    \n    # Setup output video writer if needed\n    out = None\n    if output_path:\n        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n        out = cv2.VideoWriter(output_path, fourcc, fps // skip_frames, (width, height))\n        print(f\"Output will be saved to: {output_path}\")\n    \n    # Processing variables\n    frame_predictions = []\n    frame_count = 0\n    processed_frames = 0\n    total_detections = 0\n    \n    try:\n        pbar = tqdm(total=min(total_frames // skip_frames, max_frames or float('inf')), \n                   desc=\"Processing frames\")\n        \n        while True:\n            ret, frame = cap.read()\n            if not ret:\n                break\n            \n            frame_count += 1\n            \n            # Skip frames if needed\n            if frame_count % skip_frames != 0:\n                continue\n            \n            # Check max frames limit\n            if max_frames and processed_frames >= max_frames:\n                break\n            \n            # Save frame temporarily\n            temp_frame_path = os.path.join(temp_dir, f\"temp_frame_{processed_frames:06d}.jpg\")\n            cv2.imwrite(temp_frame_path, frame)\n            \n            # Use existing predict_image function\n            predictions = predict_image(model, temp_frame_path, device, confidence_threshold)\n            \n            frame_predictions.append({\n                'frame_number': frame_count,\n                'predictions': predictions,\n                'timestamp': frame_count / fps\n            })\n            \n            if out:\n                # Draw predictions on frame using OpenCV (simple version)\n                annotated_frame = frame.copy()\n                for box, score in zip(predictions['boxes'], predictions['scores']):\n                    x1, y1, x2, y2 = box.astype(int)\n                    cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n                    cv2.putText(annotated_frame, f'Person: {score:.2f}', (x1, y1-10), \n                               cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n                \n                # Add frame info\n                cv2.putText(annotated_frame, f'Detections: {len(predictions[\"boxes\"])}', \n                           (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n                \n                out.write(annotated_frame)\n            \n            # Clean up temporary frame\n            os.remove(temp_frame_path)\n            \n            total_detections += len(predictions['boxes'])\n            processed_frames += 1\n            pbar.update(1)\n            \n            # Update progress bar description\n            if processed_frames % 30 == 0:  # Update every 30 frames\n                avg_detections = total_detections / processed_frames\n                pbar.set_description(f\"Processing frames (avg: {avg_detections:.1f} det/frame)\")\n        \n        pbar.close()\n    \n    finally:\n        cap.release()\n        if out:\n            out.release()\n        # Clean up temp directory\n        if os.path.exists(temp_dir):\n            import shutil\n            shutil.rmtree(temp_dir)","metadata":{"trusted":true,"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-07-06T11:39:24.888361Z","iopub.execute_input":"2025-07-06T11:39:24.889133Z","iopub.status.idle":"2025-07-06T11:39:24.907713Z","shell.execute_reply.started":"2025-07-06T11:39:24.889105Z","shell.execute_reply":"2025-07-06T11:39:24.906368Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"results = predict_video(\n    model = model,\n    video_path = INPUT_FILE_PATH,\n    device = device,\n    confidence_threshold = 0.5,\n    output_path = 'output_with_detections.mp4',\n    skip_frames = 2  \n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T11:39:38.836039Z","iopub.execute_input":"2025-07-06T11:39:38.836427Z","iopub.status.idle":"2025-07-06T12:07:17.101698Z","shell.execute_reply.started":"2025-07-06T11:39:38.836391Z","shell.execute_reply":"2025-07-06T12:07:17.099549Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}